{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идеи:\n",
    "- Применить идеалогию изображений: использовать вектора слов как строки или столбцы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель оценки заголовка\n",
    "\n",
    "Введенные или сгенерированные заголовки нужно как-то оценивать. Так как главной целью обычно является увеличение количества просмотров, то в качестве критерия можно использовать число просмотров у ранее опубликованных статей.\n",
    "\n",
    "Таким образом, перед нами стоит задача регрессии: на входе заголовок, на выходе число (балл от 0 до 10 с точностью 0.1).\n",
    "\n",
    "В качестве библиотеки, реализующей модель русского языка, мы используем spaCy (в первую очередь из соображений скорости)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Подготовка датасета\n",
    "\n",
    "В нашем распоряжении множество данных, полученных в результате парсинга. Объединим их в один большой датасет для построения модели оценки заголовков. Для его построения нам нужны те датасеты (сайты), для которых имеется информация о количестве просмотров статьи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# standard libraries\n",
    "import pickle\n",
    "\n",
    "# data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# data processing progress bar\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# обработка естественного языка\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# библиотеки для машинного и глубокого обучения\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# пути к датасетам\n",
    "DATASETS_PATH = \"/home/leo/DATASETS\"\n",
    "TOKENIZED_TITLES_PATH = f\"{DATASETS_PATH}/tokenized_titles.pickle\"\n",
    "\n",
    "# словарь с источником данных и их характеристиками\n",
    "with open('../sources.pickle', 'rb') as f:\n",
    "    sources = pickle.load(f)\n",
    "    \n",
    "# Токенизация большого числа заголовков — затратная по времени операция.\n",
    "# Поэтому предварительно токенизированные заголовки хранятся в виде\n",
    "# сжатого датафрайма\n",
    "tokenized_titles = pd.read_pickle(TOKENIZED_TITLES_PATH, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_titles.to_pickle(path=TOKENIZED_TITLES_PATH, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# соединяем датасеты в один общий датасет с именем df\n",
    "dfs = dict()\n",
    "\n",
    "for source in sources:\n",
    "    dfs[source] = pd.read_csv(f\"{DATASETS_PATH}/{source}.csv\",\n",
    "                              index_col=0,\n",
    "                              parse_dates=['post_time', 'parse_time'])\n",
    "    dfs[source]['source'] = source\n",
    "    \n",
    "df = pd.concat(dfs[key] for key in dfs)\n",
    "\n",
    "# удаляем дубликаты\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# преобразуем количество просмотров к текстовому значению\n",
    "df.views_num = df.views_num.apply(lambda x: int(''.join(filter(str.isdigit, str(x)))))\n",
    "\n",
    "# приводим число просмотров к нормированной логарифмической шкале\n",
    "df.views_num = np.log(df.views_num)/np.log(df.views_num.max())\n",
    "\n",
    "# удаляем записи без просмотров (обычно это закрытые и недоступные статьи)\n",
    "df = df.drop(df[df.views_num == np.NINF].index)\n",
    "\n",
    "# объединим датасет с токенизированные заголовки\n",
    "df = df.join(tokenized_titles)\n",
    "df = df.loc[~df.index.duplicated(keep='last')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Векторизация текста и построение модели на PyTorch\n",
    "\n",
    "Начнем с базовой модели: сопоставим каждому заголовку векторное представление в виде набора векторов для слов, дополним векторные представления нулевыми векторами, сложим их в трехмерный тензор, обучим модель для оценки заголовков и проверим качество скользящим контролем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В качестве конечных данных нам нужны лишь сведения\n",
    "# о токенах заголовков и количестве просмотров статей.\n",
    "Xy = df[['doc', 'views_num']]\n",
    "\n",
    "# удаляем пропущенные значения, если таковые есть\n",
    "Xy = Xy.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd754fa113124b5c9af23f9765911f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228674 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_tokens_in_title = Xy.doc.apply(len).max()\n",
    "cols_num = 96\n",
    "\n",
    "def make_stack(doc):\n",
    "    # представляем заголовок как набор векторов\n",
    "    words_stack = np.vstack(word.vector for word in doc)\n",
    "    \n",
    "    # дополняем плоскость нулями\n",
    "    zeros_rows_num = max_tokens_in_title-words_stack.shape[0]\n",
    "    zeros_stack = np.zeros((zeros_rows_num, cols_num))\n",
    "    plate_stack = np.vstack([words_stack, zeros_stack])\n",
    "    return plate_stack\n",
    "\n",
    "Xy['stack'] = Xy.doc.progress_apply(lambda x: make_stack(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Xy.views_num.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.from_numpy(y_train).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6151, 0.7249, 0.3381,  ..., 0.5632, 0.3411, 0.5415])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Проверка дополнительных гипотез"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# генерация дополнительных признаков\n",
    "# Xy.loc[:, ['title']] = Xy.title.apply(str)\n",
    "\n",
    "# Xy.loc[:, ['doc']] = Xy.title.progress_apply(nlp)\n",
    "\n",
    "# длина заголовка в символах\n",
    "# Xy.loc[:, ['len']] = Xy.title.apply(len)\n",
    "\n",
    "# количество токенов\n",
    "# Xy.loc[:, ['tokens_num']] = Xy.tokens.apply(lambda x: len(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
