{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идеи:\n",
    "- Применить идеалогию изображений: использовать вектора слов как строки или столбцы\n",
    "- Нужна функция, которая возвращает N матриц X для обучения\n",
    "- https://www.reddit.com/r/MLQuestions/comments/e8kyc3/finetuning_bert_for_a_regression_task/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель оценки заголовка\n",
    "\n",
    "Введенные или сгенерированные заголовки нужно как-то оценивать. Так как главной целью обычно является увеличение количества просмотров, то в качестве критерия можно использовать число просмотров у ранее опубликованных статей.\n",
    "\n",
    "Таким образом, перед нами стоит задача регрессии: на входе заголовок, на выходе число (балл от 0 до 10 с точностью 0.1).\n",
    "\n",
    "В качестве библиотеки, реализующей модель русского языка, мы используем spaCy (в первую очередь из соображений скорости).\n",
    "\n",
    "Для работы с текстами мы используем библиотеку [Transformers](https://huggingface.co/transformers/), обладающую высокой эффективностью для задач «понимания» текста (NLU) и его генерации (NLG). Библиотека предоставляет удобный интерфес для работы с предобученными NLP-моделями на основе архитектуры transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Подготовка датасета\n",
    "\n",
    "В нашем распоряжении множество данных, полученных в результате парсинга. Объединим их в один большой датасет для построения модели оценки заголовков. Для его построения нам нужны те датасеты (сайты), для которых имеется информация о количестве просмотров статьи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# standard libraries\n",
    "import pickle\n",
    "import io\n",
    "\n",
    "# data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# make numpy printouts easier to read\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# data processing progress bar\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# обработка естественного языка\n",
    "#import spacy\n",
    "#nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# библиотеки для машинного и глубокого обучения\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "model_name = 'DeepPavlov/rubert-base-cased'\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пути к датасетам\n",
    "DATASETS_PATH = \"/home/leo/DATASETS\"\n",
    "# TOKENIZED_TITLES_PATH = f\"{DATASETS_PATH}/tokenized_titles.pickle\"\n",
    "\n",
    "# словарь с источником данных и их характеристиками\n",
    "with open('../sources.pickle', 'rb') as f:\n",
    "    sources = pickle.load(f)\n",
    "    \n",
    "# Токенизация большого числа заголовков — затратная по времени операция.\n",
    "# Поэтому предварительно токенизированные заголовки хранятся в виде\n",
    "# сжатого датафрайма\n",
    "#tokenized_titles = pd.read_pickle(TOKENIZED_TITLES_PATH, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [3, 8, 9]:\n",
    "#     spacy.displacy.render(tokenized_titles.iloc[i], style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_titles.to_pickle(path=TOKENIZED_TITLES_PATH, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# соединяем датасеты в один общий датасет с именем df\n",
    "dfs = dict()\n",
    "\n",
    "for source in sources:\n",
    "    dfs[source] = pd.read_csv(f\"{DATASETS_PATH}/{source}.csv\",\n",
    "                              index_col=0,\n",
    "                              parse_dates=['post_time', 'parse_time'])\n",
    "    dfs[source]['source'] = source\n",
    "    \n",
    "df = pd.concat(dfs[key] for key in dfs)\n",
    "\n",
    "# удаляем дубликаты\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# преобразуем количество просмотров к текстовому значению\n",
    "df.views_num = df.views_num.apply(lambda x: int(''.join(filter(str.isdigit, str(x)))))\n",
    "\n",
    "# удаляем записи без просмотров (обычно это закрытые и недоступные статьи)\n",
    "df = df.drop(df[df.views_num == 0.0].index)\n",
    "\n",
    "# объединим датасет с токенизированные заголовки\n",
    "# df = df.join(tokenized_titles)\n",
    "df = df.loc[~df.index.duplicated(keep='last')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Векторизация текста и построение модели на PyTorch\n",
    "\n",
    "Начнем с базовой модели: сопоставим каждому заголовку векторное представление в виде набора векторов для слов, дополним векторные представления нулевыми векторами, сложим их в трехмерный тензор, обучим модель для оценки заголовков и проверим качество скользящим контролем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В качестве конечных данных нам нужны лишь сведения\n",
    "# о токенах заголовков и количестве просмотров статей.\n",
    "Xy = df[['title', 'views_num']]\n",
    "\n",
    "# удаляем пропущенные значения, если таковые есть\n",
    "Xy = Xy.dropna(axis=0)\n",
    "\n",
    "# разобьем все группы на 21 класс от 0 до 20\n",
    "# по числу оценок от 0.0 до 10.0 с шагом 0.5\n",
    "Xy = Xy.sort_values(by='views_num')\n",
    "score = np.linspace(0, 1, Xy.shape[0])\n",
    "Xy['score'] = score\n",
    "Xy = Xy.drop(columns='views_num')\n",
    "\n",
    "# представим данные в виде кортежей (токенизированный текст, метка)\n",
    "# data = Xy.apply(lambda row: (row['doc'], row['score']), axis=1).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = Xy.title.apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# в библиотеке transformers таск sentiment-analysis\n",
    "# соответствует TextClassificationPipeline\n",
    "classifier = pipeline(task=\"sentiment-analysis\",\n",
    "                      model=model,\n",
    "                      tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_batch = tokenizer(\n",
    "    Xy.title.to_list(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=max_length,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "https://habr.com/ru/post/91894     Продолжаем делать обзоры игр для Linux или сво...\n",
       "https://habr.com/ru/post/29361                Наши нигде не упустят шанса отличиться\n",
       "https://habr.com/ru/post/91849     Обзор игры America's Army: Special Forces для ...\n",
       "https://habr.com/ru/post/104954                      Обзор игры «Alchemia» для Linux\n",
       "https://habr.com/ru/post/30612                             Google Translation Center\n",
       "                                                         ...                        \n",
       "https://habr.com/ru/post/50764               Jailbreak iPod Touch 2G — redsn0w вышел\n",
       "https://habr.com/ru/post/109263          Домены разблокированы, а кто бы сомневался?\n",
       "https://habr.com/ru/post/49151                                    Новый qutim из SVN\n",
       "https://habr.com/ru/post/36359     Еженедельный подкаст от Umputun (US, Чикаго) #173\n",
       "https://habr.com/ru/post/38997                                             Хабра RSS\n",
       "Name: title, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"rubert-base-cased\"  # https://huggingface.co/DeepPavlov/rubert-base-cased\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(limit=0, split=0.8):\n",
    "    train_data = data\n",
    "    np.random.shuffle(train_data)\n",
    "    train_data = train_data[-limit:]\n",
    "    texts, labels = zip(*train_data)\n",
    "    cats = [{'POSITIVE': bool(y)} for y in labels]\n",
    "    split = int(len(train_data) * split)\n",
    "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy['tuples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens_in_title = Xy.doc.apply(len).max()\n",
    "cols_num = 96\n",
    "\n",
    "def make_stack(doc):\n",
    "    # представляем заголовок как набор векторов\n",
    "    words_stack = np.vstack(word.vector for word in doc)\n",
    "    \n",
    "    # дополняем плоскость нулями\n",
    "    zeros_rows_num = max_tokens_in_title-words_stack.shape[0]\n",
    "    zeros_stack = np.zeros((zeros_rows_num, cols_num))\n",
    "    plate_stack = np.vstack([words_stack, zeros_stack])\n",
    "    return plate_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Xy.views_num.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.from_numpy(y_train).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Проверка дополнительных гипотез"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# генерация дополнительных признаков\n",
    "# Xy.loc[:, ['title']] = Xy.title.apply(str)\n",
    "\n",
    "# Xy.loc[:, ['doc']] = Xy.title.progress_apply(nlp)\n",
    "\n",
    "# длина заголовка в символах\n",
    "# Xy.loc[:, ['len']] = Xy.title.apply(len)\n",
    "\n",
    "# количество токенов\n",
    "# Xy.loc[:, ['tokens_num']] = Xy.tokens.apply(lambda x: len(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
