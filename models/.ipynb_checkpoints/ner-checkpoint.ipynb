{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    data_path='/content/gdrive/My Drive/Colab Notebooks/title/data'\n",
    "except:\n",
    "    data_path='../../DATASETS/IT_TEXTS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = pd.read_feather(f'{data_path}/META/it_articles_ru.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = Xy.sort_values(by='views_num', ascending=False)[['url', 'title']]\n",
    "# _.reset_index(inplace=True, drop=True)\n",
    "# #_.to_feather(f'{data_path}/titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_titles_str = '\\n'.join(_.title.to_list()[:10000])\n",
    "# with open(f'{data_path}/all_titles.txt', \"w\") as text_file:\n",
    "#     text_file.write(all_titles_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used [CoNLL format](https://www.signll.org/conll/) ([StackOverflow discussion](https://stackoverflow.com/questions/27416164/what-is-conll-data-format)) that also used by Label Studio and by example at HuggingFace documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll(file_path):\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    raw_text = file_path.read_text(encoding='utf-8').strip()\n",
    "    raw_docs = re.split(r'\\n\\n', raw_text)\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    for doc in raw_docs:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in doc.split('\\n'):\n",
    "            try:\n",
    "                token, tag = line.split('-X- _')\n",
    "                tokens.append(token.strip())\n",
    "                tags.append(tag.strip())\n",
    "            except ValueError:\n",
    "                pass\n",
    "        token_docs.append(tokens)\n",
    "        tag_docs.append(tags)\n",
    "        \n",
    "    return token_docs, tag_docs\n",
    "        \n",
    "texts, tags = read_conll(f'{data_path}/PREPROCESSING/titles.conll')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve read the data in, let’s create a train/validation split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s create encodings for our tokens and tags. For the tags, we can start by just create a simple mapping which we’ll use in a moment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = set(tag for doc in tags for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode the tokens, we’ll use a pre-trained `DeepPavlov/rubert-base-cased` tokenizer. We can tell the tokenizer that we’re dealing with ready-split tokens rather than full sentence strings by passing `is_split_into_words=True`. We’ll also pass `padding=True` and to pad the sequences to be the same length. Lastly, we can tell the model to return information about the tokens which are split by the wordpiece tokenization process, which we will need in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c08d9eeb39040b0a892afe34927f0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=213450.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c6e97cbae94a1892b2449a0460dce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=435797.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bd9b7c32404d7ca1b7b8bccf9f58bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=29.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so now our tokens are nicely encoded in the format that they need to be in to feed them into our model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def encode_tags(tags, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "val_labels = encode_tags(val_tags, val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CONLLDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = CONLLDataset(train_encodings, train_labels)\n",
    "val_dataset = CONLLDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd7a174c4fd4b72946b92b43dff66e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=411.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7ce6d4593943278f51510ceb725ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=263273408.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DistilBertForTokenClassification\n",
    "model = DistilBertForTokenClassification.from_pretrained('distilbert-base-cased',\n",
    "                                                         num_labels=len(unique_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2f55d231a54891b1dc8ac38cab10f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=642.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24bbe846d7cf4dab9d8acdccde029429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=711456796.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(119547, 768, padding_idx=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_name,\n",
    "                                                      num_labels=2,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "texts\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "* [Token Classification with W-NUT Emerging Entities](https://huggingface.co/transformers/custom_datasets.html?highlight=named%20entity%20recognition#token-classification-with-w-nut-emerging-entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1477,\n",
       " 'annotations': [{'id': 1501,\n",
       "   'completed_by': {'id': 1,\n",
       "    'email': 'leva.matyushkin@gmail.com',\n",
       "    'first_name': 'Leo',\n",
       "    'last_name': 'Matyushkin'},\n",
       "   'state': {},\n",
       "   'result': [{'value': {'start': 16,\n",
       "      'end': 36,\n",
       "      'text': 'платежные технологии',\n",
       "      'labels': ['TECH']},\n",
       "     'id': 'CFSQHFanLE',\n",
       "     'from_name': 'label',\n",
       "     'to_name': 'text',\n",
       "     'type': 'labels'},\n",
       "    {'value': {'start': 0, 'end': 3, 'text': 'Как', 'labels': ['PROFIT']},\n",
       "     'id': '1GeN7olSvI',\n",
       "     'from_name': 'label',\n",
       "     'to_name': 'text',\n",
       "     'type': 'labels'},\n",
       "    {'value': {'start': 50,\n",
       "      'end': 85,\n",
       "      'text': 'предприятиями общественного питания',\n",
       "      'labels': ['TECH']},\n",
       "     'id': '6k_PJzBp6a',\n",
       "     'from_name': 'label',\n",
       "     'to_name': 'text',\n",
       "     'type': 'labels'},\n",
       "    {'value': {'start': 86,\n",
       "      'end': 99,\n",
       "      'text': 'по всему миру',\n",
       "      'labels': ['AUD']},\n",
       "     'id': 'rxrvBPO0W_',\n",
       "     'from_name': 'label',\n",
       "     'to_name': 'text',\n",
       "     'type': 'labels'}],\n",
       "   'was_cancelled': False,\n",
       "   'ground_truth': False,\n",
       "   'created_at': '2021-05-18T08:21:15.296498Z',\n",
       "   'updated_at': '2021-05-18T08:21:33.438234Z',\n",
       "   'lead_time': 7020.065,\n",
       "   'prediction': {},\n",
       "   'result_count': 0,\n",
       "   'task': 1477}],\n",
       " 'predictions': [],\n",
       " 'file_upload': 'all_titles_wm8jtHw.txt',\n",
       " 'data': {'ner': 'Как современные платежные технологии используются предприятиями общественного питания по всему миру'},\n",
       " 'meta': {},\n",
       " 'created_at': '2021-05-09T15:46:57.153327Z',\n",
       " 'updated_at': '2021-05-18T08:21:33.420318Z',\n",
       " 'project': 2}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
