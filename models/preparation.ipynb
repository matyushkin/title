{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš¿ Preparing Data for Colab Models: title and summary scoring\n",
    "\n",
    "Let's prepare previously parsed data. This notebook is intended for converting the dataset containing information about articles to several smaller datasets necessary for training and testing the scoring model. The goal of the project is to increase the number of views, so we use the number of views of published articles as a criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm \n",
    "tqdm.pandas()  # data processing progress bar\n",
    "\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "# directory where all datasets are placed\n",
    "DATASETS_PATH = \"../../DATASETS/IT_TEXTS\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read Dataframe with Titles and Views Numbers\n",
    "\n",
    "Now we have a lot of data obtained as a result of web scraping, combined into one large dataset. For reading we use `read_df()` functions and for occasional work with dataset and saving results â€“ `write_df()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(filename = 'it_articles_ru'):\n",
    "    \"\"\"Read total dataframe with information about all sources\"\"\"\n",
    "    df = pd.read_feather(f'{DATASETS_PATH}/META/{filename}.feather')\n",
    "    \n",
    "    # feather format doesn't work with str indices\n",
    "    df = df.set_index('url')  \n",
    "    \n",
    "    # convinient order of columns\n",
    "    df = df[['title', 'summary', 'views_num',\n",
    "             'post_time', 'parse_time',\n",
    "             'likes_num', 'favs_num', 'comments_num',\n",
    "             'source', 'fulltext_saved']]\n",
    "    \n",
    "    # parse timing cols ad datetime\n",
    "    for col in ('post_time', 'parse_time'):\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    \n",
    "    df['fulltext_saved'] = df['fulltext_saved'].astype(bool)\n",
    "    \n",
    "    # drop dupicates, keep last of all duplicate rows\n",
    "    df = df.loc[~df.index.duplicated(keep='last')]\n",
    "    \n",
    "    # drop non-public articles\n",
    "    df = df.drop(df[df.views_num == 0.0].index)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def write_df(df, filename = 'it_articles_ru'):\n",
    "    \"\"\"Write total dataframe with information about all sources\"\"\"\n",
    "    df = df.reset_index() # feather format doesn't work with str indices\n",
    "    \n",
    "    for col in ('post_time', 'parse_time'):\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    \n",
    "    # for space economy let's convert types\n",
    "    df.views_num = df.views_num.astype('Int32')\n",
    "    for col in ('likes_num', 'favs_num', 'comments_num'):\n",
    "        df[col] = df[col].astype('Int16')\n",
    "    \n",
    "    # zstd compression is best for our str content\n",
    "    df.to_feather(f'{DATASETS_PATH}/META/{filename}.feather',\n",
    "                  compression='zstd')\n",
    "    \n",
    "    # convert time to simple iso format strings\n",
    "    df.post_time = df.post_time.apply(lambda x: x.isoformat().replace('+03:00', ''))\n",
    "    df.parse_time = df.parse_time.apply(lambda x: x.date().isoformat())\n",
    "    \n",
    "    print('Main datafame is saved.')\n",
    "    \n",
    "df = read_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 323265 entries, https://tproger.ru/articles/7-prakticheskih-zadanij-s-sobesedovanija-na-poziciju-junior-java-developer/ to https://dev.by/news/besarab-chto-chitat\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count   Dtype         \n",
      "---  ------          --------------   -----         \n",
      " 0   title           323265 non-null  object        \n",
      " 1   summary         322943 non-null  object        \n",
      " 2   views_num       308037 non-null  Int32         \n",
      " 3   post_time       322643 non-null  datetime64[ns]\n",
      " 4   parse_time      322756 non-null  datetime64[ns]\n",
      " 5   likes_num       217990 non-null  Int16         \n",
      " 6   favs_num        217990 non-null  Int16         \n",
      " 7   comments_num    316750 non-null  Int16         \n",
      " 8   source          323265 non-null  object        \n",
      " 9   fulltext_saved  323265 non-null  bool          \n",
      "dtypes: Int16(3), Int32(1), bool(1), datetime64[ns](2), object(3)\n",
      "memory usage: 27.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of articles by source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "habr            214949\n",
       "vc               46635\n",
       "xakep            36752\n",
       "devby            14717\n",
       "dou               5680\n",
       "proglib           1750\n",
       "tproger           1293\n",
       "tinkoff            656\n",
       "thecode            509\n",
       "digitalocean       324\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ML Correction of Underestimated Number of Views\n",
    "The number of views on sites sometimes significantly lags behind the expected or is not always calculated correctly. For example, for new articles, or articles that changed the publicity status. This is especially noticeable when the number of views is less than the number of likes and bookmarks. To correct such values, we build a simple SGD regression model on data we can trust. Next, we extrapolate the result to \"suspicious\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "df['timedelta'] = (df.parse_time - df.post_time).apply(lambda x: x.total_seconds())*1e-6\n",
    "df_tmp = df[['likes_num', 'favs_num', 'comments_num', 'views_num', 'source', 'timedelta']].dropna()\n",
    "\n",
    "# for categorical data (source feature)\n",
    "df_tmp = pd.get_dummies(df_tmp)\n",
    "df_tmp['suspicious'] = [False]*df_tmp.shape[0]\n",
    "for col in ('likes', 'favs', 'comments'):\n",
    "    df_tmp['suspicious'] += df_tmp[f'{col}_num'] > 0.1*df_tmp['views_num']\n",
    "\n",
    "df_tmp_susp = df_tmp[df_tmp['suspicious'] == True]\n",
    "df_tmp = df_tmp[df_tmp['suspicious'] == False]\n",
    "\n",
    "df_tmp = df_tmp.drop(columns=['suspicious'])\n",
    "df_tmp_susp = df_tmp_susp.drop(columns=['suspicious'])\n",
    "\n",
    "y = df_tmp['views_num']\n",
    "X = df_tmp.drop(columns=['views_num'])\n",
    "reg = make_pipeline(StandardScaler(),\n",
    "                    RandomForestRegressor(n_jobs=20))\n",
    "reg.fit(X, y)\n",
    "df_tmp_susp['views_num'] = reg.predict(df_tmp_susp.drop(columns=['views_num']))\n",
    "df_tmp_susp['views_num'] = df_tmp_susp['views_num'].apply(round)\n",
    "df_tmp = pd.concat([df_tmp, df_tmp_susp])\n",
    "\n",
    "df.update(df_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Variable Conversion: Views to Rating\n",
    "\n",
    "Let's create datasets with the minimum required set of columns, corresponding to the model. We will intentionally code both the title and the summary together, since this is exactly the information that the user sees.\n",
    "\n",
    "Now we will sort the articles by the number of views and set each position in the top 20% to 1, and for the rest â€“ 0. So the closer the classification score to 1, the more likely the article will receive more views.\n",
    "\n",
    "It should be noted that we cannot take into consideration articles that were published in the last month, unless they immediately fall into `False` section. Usually, recently published articles have received only few views, but this does not mean that they are not interesting. Often, such articles have simply not been indexed yet.\n",
    "The same applies to the fact that an article published five years ago has a higher chance of being read more times than one that was published six months ago. To equalize the \"chances\" of such articles, we divide the timeline into several indipendent segments, where we sort items by the number of views. Hereinafter, the word \"time\" does not mean an ordinary date, but the difference between the time of publication of the article and the time of parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Xy_preprocessing(df, true_fraction = 0.2):\n",
    "    Xy = df[['post_time', 'parse_time', 'title', 'summary', 'views_num']]\n",
    "    Xy.reset_index(inplace=True)\n",
    "\n",
    "    # not interested in NA views_num, post_time and parse_time\n",
    "    Xy.dropna(subset=['views_num', 'parse_time', 'post_time'], inplace=True)\n",
    "\n",
    "    # drop dupicates of title + summary\n",
    "    Xy.drop_duplicates(subset=['title', 'summary'],\n",
    "                       keep='last', inplace=True)\n",
    "\n",
    "    Xy['delta'] = (Xy['parse_time']-Xy['post_time']).apply(lambda x: int(x.days))\n",
    "    Xy.drop(columns=['parse_time', 'post_time'], inplace=True)\n",
    "\n",
    "    # we don't save time of parsing, so delta can be \"-1 days\" but actually it's 0\n",
    "    Xy.loc[Xy.delta < 0, 'delta'] = 0\n",
    "\n",
    "    Xy.sort_values(by='delta', inplace=True)\n",
    "    Xy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # let's divide the records into blocks that are multiples of 100\n",
    "    # start block_num is month items length\n",
    "    block_num = round(Xy.loc[Xy.delta < 30, :].shape[0]/100)*100\n",
    "\n",
    "    # others bloack are 2**n progression nums\n",
    "    # -1 used for not making \"old tail\" too small\n",
    "    bins_num = round(np.log2(Xy.shape[0]/block_num))-1\n",
    "\n",
    "    # slices_indexes for bins is cumulative sum for progression\n",
    "    # this approach makes it possible to consider more modern\n",
    "    # articles with a higher priority, and treat old ones more strictly.\n",
    "    bins = np.cumsum([2**n*block_num for n in range(bins_num)])\n",
    "    bins = np.insert(bins, 0, 0)\n",
    "    bins = np.append(bins, Xy.shape[0])\n",
    "\n",
    "    Xy['interval'] = pd.cut(Xy.index, bins,\n",
    "                            include_lowest=True)\n",
    "\n",
    "    Xy.sort_values(['interval', 'views_num'],\n",
    "                   ascending=[True, False],\n",
    "                   inplace=True)\n",
    "\n",
    "    Xy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # true fraction is an adjustable value\n",
    "    arr = (bins + np.diff(bins, append=0)*true_fraction)\n",
    "    bins = np.ravel([bins, arr],'F')[:-1]\n",
    "\n",
    "    Xy['interval_01'] = pd.cut(Xy.index, bins,\n",
    "                               include_lowest=True)\n",
    "\n",
    "    classes = {interval: 1 if i%2 == 0 else 0 for i, interval in enumerate(Xy.interval_01.unique())}\n",
    "    Xy.replace({'interval_01': classes}, inplace=True)\n",
    "    Xy.drop(columns = ['views_num', 'delta', 'interval'], inplace=True)\n",
    "    Xy.rename(columns={'interval_01': 'class'}, inplace=True)\n",
    "    \n",
    "    return Xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = Xy_preprocessing(df)\n",
    "Xy.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy.to_feather(f'{DATASETS_PATH}/PREPROCESSING/Xy.feather',\n",
    "              compression='zstd')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
