{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Текущие задачи**:\n",
    "- Реализация опроса новых данных о свежих статьях\n",
    "- Описать `requirements` или сделать Docker-образ\n",
    "- Выгружать датасет из памяти после того, как он был считан. Составлять новый датасет, выводить в csv и конкатенировать сами csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Парсер\n",
    "\n",
    "Реализованы парсеры данных, собраны тексты и метаинформация статей следующих ресурсов:\n",
    "- Хабрахабр https://habr.com/\n",
    "- Типичный программист https://tproger.ru/\n",
    "- DOU\n",
    "- Библиотека программиста\n",
    "- Код\n",
    "- Русскоязычные туториалы DigitalOcean\n",
    "\n",
    "\n",
    "Также реализован механизм докачивания новых статей, чтобы учесть появление новых технологий. Чтобы не создавать повышенную нагрузку на серверы ресурсов и не вызывать их подозрений, опрашивание ресурсов происходит поочередно. Методы парсеров содержатся внутри класса `Parser`, методы оркестровки (планирование и общий пайплайн вызова парсеров) – в классе `ParserComposer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Системные требования:\n",
    "- Каталог с датасетами\n",
    "- Дравйвер `/usr/bin/chromedriver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library\n",
    "import random\n",
    "import datetime\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "# conections\n",
    "import requests\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "# data science \n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "sources = {\n",
    "    'tproger':\n",
    "    {\n",
    "        'base_url': 'https://tproger.ru/',\n",
    "        'first_page_url': 'https://tproger.ru/page/1/',\n",
    "        'find_all_args': ('a', {'class':'article-link'})\n",
    "    },\n",
    "    'habr':\n",
    "    {\n",
    "        'base_url': 'https://habr.com/',\n",
    "        'first_page_url': 'https://habr.com/ru/page1/',\n",
    "        'find_all_args': ('a', {'class':'post__title_link'})\n",
    "    },\n",
    "#     'proglib':\n",
    "#     {\n",
    "#         'base_url': 'https://proglib.io/',\n",
    "#         'first_page_url': 'https://proglib.io/',\n",
    "#         'btn_class': 'load-more active',\n",
    "#     }\n",
    "}\n",
    "\n",
    "proxies = {\n",
    "    'http': 'socks5h://127.0.0.1:9050',\n",
    "    'https': 'socks5h://127.0.0.1:9050'\n",
    "}\n",
    "\n",
    "\n",
    "DATASETS_PATH = '/home/leo/DATASETS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_to_num(s):\n",
    "    if type(s) is str:\n",
    "        s = s.replace(',', '.')\n",
    "        if 'k' in s:\n",
    "            s = s.replace('k', 'e3')\n",
    "    return round(float(s))\n",
    "\n",
    "class Parser:\n",
    "    def __init__(self, source_name):\n",
    "        self.source_name = source_name\n",
    "        self.source = sources[source_name]\n",
    "        self.df = pd.read_csv(f\"{DATASETS_PATH}/{source_name}.csv\",\n",
    "                         index_col=0,\n",
    "                         parse_dates=['post_time', 'parse_time'])\n",
    "\n",
    "    \n",
    "    def check_new_articles(self, nurls, proxies=proxies):\n",
    "        '''Проверяет, появились ли на ресурсе новые публикации,\n",
    "        и если появились, заносит их в план на скачивание.'''\n",
    "        all_new_urls = nurls[self.source_name]\n",
    "        page_count = 1\n",
    "        source = self.source\n",
    "\n",
    "        attempts_counts = 0\n",
    "        while True:\n",
    "            try:\n",
    "                page_url = source['first_page_url'].replace('1', str(page_count))\n",
    "                page = requests.get(page_url,\n",
    "                                    headers=headers,\n",
    "                                    proxies=proxies,\n",
    "                                    stream=False)\n",
    "                soup = BeautifulSoup(page.text, 'html.parser')\n",
    "                urls = {url['href'] for url in soup.find_all(*source['find_all_args'])}\n",
    "                if not urls:\n",
    "                    # the thread processes the case when captcha and no urls\n",
    "                    print(\"Changing proxy because of no urls or CAPTCHA...\")\n",
    "                    attempts_counts += 1\n",
    "                    if attempts_counts >= 2:\n",
    "                        print(f\"Data parsing of {self.source_name} is stopped at {page_url}.\")\n",
    "                        break\n",
    "                    proxies = None\n",
    "                    continue\n",
    "                old_urls = set(self.df.index.to_list())\n",
    "                new_urls = urls - old_urls\n",
    "                all_new_urls = all_new_urls | new_urls\n",
    "                nurls[self.source_name] = all_new_urls\n",
    "                if len(new_urls) > 0:\n",
    "                    print(f\"{len(new_urls)} urls are collected from {page_url}\")\n",
    "                if (len(new_urls) == 0) or (len(urls) == 0):    \n",
    "                    print(f\"{len(all_new_urls)} new urls are saved for {source['base_url']}\")\n",
    "                    break\n",
    "                else:\n",
    "                    page_count += 1\n",
    "            except ConnectionError:\n",
    "                print(\"Problems with connection...\")\n",
    "                continue\n",
    "        with open('not_processed_urls.pickle', 'wb') as f:\n",
    "            pickle.dump(nurls, f)\n",
    "        return nurls\n",
    "\n",
    "\n",
    "    def save_new_data(self, url, data, full_text):\n",
    "        '''Сохраняет новые данные в датафрейм, а текст -- в отдельный файл'''\n",
    "        data['filename'] = str(uuid.uuid5(uuid.NAMESPACE_DNS, url))\n",
    "        new_row = pd.DataFrame(data, index=[url])\n",
    "        if url not in self.df:\n",
    "            self.df = self.df.append(new_row)\n",
    "        else:\n",
    "            self.df.iloc[url] = new_row\n",
    "        \n",
    "        filepath = f\"{DATASETS_PATH}/{self.source_name}/{data['filename']}\"\n",
    "        if not os.path.exists(filepath):\n",
    "            with open(filepath, 'w') as f:\n",
    "                full_text = re.sub('\\n+', '\\n\\n', full_text).strip()\n",
    "                f.write(full_text)\n",
    "                #print(f\"File {self.source_name}/{data['filename']} is saved.\")\n",
    "        else:\n",
    "            pass\n",
    "            #print(f\"File {self.source_name}/{data['filename']} is already exist.\")\n",
    "            \n",
    "    def save_df(self):\n",
    "        self.df.to_csv(f\"{DATASETS_PATH}/{self.source_name}.csv\")\n",
    "            \n",
    "    def concat_csv(self):\n",
    "        '''\n",
    "        Соединяет старый csv и тот, что получен из новых данных.\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "\n",
    "class HabrParser(Parser):\n",
    "    '''Парсер для сайта habr.com'''\n",
    "    def update(self, url):\n",
    "        '''Скачивает или обновляет данные о статье.'''\n",
    "        data = dict()\n",
    "        page = requests.get(url, headers=headers, stream=False)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        data['title'] = soup.select_one('h1').text.strip()\n",
    "        try:\n",
    "            date_time = soup.select_one('.post__time').get('data-time_published')\n",
    "            data['post_time'] = datetime.datetime.strptime(date_time, '%Y-%m-%dT%H:%MZ')\n",
    "        except AttributeError:\n",
    "            date_time = None\n",
    "        try:\n",
    "            data['views_num'] = k_to_num(soup.select_one('.post-stats__views-count').text.strip())\n",
    "        except AttributeError:\n",
    "            data['views_num'] = 0\n",
    "        try:\n",
    "            data['likes_num'] = k_to_num(soup.select_one('.js-post-vote').text.strip().replace('–', '-'))\n",
    "        except AttributeError:\n",
    "            data['likes_num'] = 0\n",
    "        try:\n",
    "            data['favs_num'] = k_to_num(soup.select_one('.bookmark__counter').text)\n",
    "        except AttributeError:\n",
    "            data['favs_num'] = 0\n",
    "        try:\n",
    "            data['comments_num'] = k_to_num(soup.select_one('.post-stats__comments-count').text)\n",
    "        except AttributeError:\n",
    "            data['comments_num'] = 0\n",
    "        data['parse_time'] = datetime.datetime.now()\n",
    "        try:\n",
    "            full_text = soup.select_one('.post__body_full').text\n",
    "        except AttributeError:\n",
    "            full_text = \"\"\n",
    "        self.save_new_data(url, data, full_text)\n",
    "\n",
    "\n",
    "class TprogerParser(Parser):\n",
    "    '''Парсер для сайта tproger.ru'''\n",
    "    def update(self, url):\n",
    "        '''Скачивает или обновляет данные о статье.'''\n",
    "        data = dict()\n",
    "        try:\n",
    "            driver = webdriver.Chrome('/usr/bin/chromedriver')\n",
    "            driver.get(url)\n",
    "            page = driver.page_source\n",
    "            element_present = EC.presence_of_element_located((By.CSS_SELECTOR, '.post-views-count'))\n",
    "            WebDriverWait(driver, timeout=10).until(element_present)\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for page to load\")\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        data['title'] = soup.h1.text.strip()\n",
    "        data['post_time'] = datetime.datetime.fromisoformat(soup.select_one('time')['content'])\n",
    "        paragraphs = soup.find_all('div', {'class':'entry-content'})[0].find_all('p')\n",
    "        try:\n",
    "            data['summary'] = paragraphs[0].text\n",
    "        except IndexError:\n",
    "            data['summary'] = ''\n",
    "        try:\n",
    "            data['views_num'] = soup.select_one('.post-views-count').text\n",
    "        except AttributeError:\n",
    "            data['views_num'] = 0\n",
    "        data['parse_time'] = datetime.datetime.now()\n",
    "        full_text = \"\\n\\n\".join(p.text for p in paragraphs[:-1])\n",
    "        self.save_new_data(url, data, full_text)    \n",
    "        \n",
    "        \n",
    "class ParserComposer:\n",
    "    def __init__(self):\n",
    "        self.parsers = [HabrParser('habr'),\n",
    "                        TprogerParser('tproger')] \n",
    "        with open('not_processed_urls.pickle', 'rb') as f:\n",
    "            self.nurls = pickle.load(f)\n",
    "        \n",
    "    def load_not_processed_articles(self):\n",
    "        '''Загружает и обрабатывает статьи что не были обработаны\n",
    "        по каким-то причинам ранее (обрыв сигнала, вызов исключения и т.д.)'''\n",
    "        \n",
    "        pairs = [(key, item) for key in self.nurls for item in self.nurls[key]]\n",
    "        random.shuffle(pairs)\n",
    "        \n",
    "        try:\n",
    "            for pair in tqdm(pairs):\n",
    "                try:\n",
    "                    parser = [parser for parser in self.parsers if parser.source_name == pair[0]][0]\n",
    "                    parser.update(pair[1])\n",
    "                except ConnectionError:\n",
    "                    continue\n",
    "        finally:\n",
    "            for parser in self.parsers:\n",
    "                parser.save_df()\n",
    "                self.nurls[parser.source_name] -= set(parser.df.index)\n",
    "            with open('not_processed_urls.pickle', 'wb') as f:\n",
    "                pickle.dump(self.nurls, f)\n",
    "    \n",
    "    \n",
    "    def update_not_old_articles(self):\n",
    "        for parser in self.parsers:\n",
    "            n = datetime.datetime.now()\n",
    "            p = parser.df.post_time.apply(lambda x: x.replace(tzinfo=None))\n",
    "            #! Второе условие должно быть по парсингу, а не дате выхода статьи\n",
    "            condition = ((n - p) <= datetime.timedelta(days=30)) & ((n - p) >= datetime.timedelta(days=2))\n",
    "            parser.not_old = set(parser.df[condition].index)\n",
    "        pairs = [(parser.source_name, url) for parser in self.parsers for url in parser.not_old]\n",
    "        random.shuffle(pairs)\n",
    "        try:\n",
    "            for pair in tqdm(pairs):\n",
    "                try:\n",
    "                    parser = [parser for parser in self.parsers if parser.source_name == pair[0]][0]\n",
    "                    parser.update(pair[1])\n",
    "                except ConnectionError:\n",
    "                    continue\n",
    "        finally:\n",
    "            for parser in self.parsers:\n",
    "                parser.save_df()\n",
    "        \n",
    "    \n",
    "    def scheduler(self):\n",
    "        '''\n",
    "        3) загружаем данные новых статей\n",
    "        -> 4) обновляем число просмотров статей, вышедших в последний месяц (post_time)\n",
    "        5) обновляем число просмотров статей, проверявшихся > месяца назад (parse_time), не относящихся к п. 2-3\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def pipeline(self):\n",
    "        '''Сортирует задачи таким образом, чтобы\n",
    "        равномерно распределить запросы к внешним ресурсам.\n",
    "        Проверяем, вышли ли новые статьи. Планируем загрузки. Обновляем датасеты.'''\n",
    "        print(\"Load articles not processed before...\")\n",
    "        self.load_not_processed_articles()\n",
    "        for parser in self.parsers:\n",
    "            print(\"Check new articles...\")\n",
    "            self.nurls = parser.check_new_articles(self.nurls)\n",
    "            print(\"Update data for not old articles...\")\n",
    "            self.update_not_old_articles()\n",
    "            \n",
    "#         with open('not_processed_urls.pickle', 'wb') as f:\n",
    "#             pickle.dump(self.nurls, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/leo/DATASETS/habr.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-99c7feaf5066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomposer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParserComposer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcomposer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a0252de7f7a6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mParserComposer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         self.parsers = [HabrParser('habr'),\n\u001b[0m\u001b[1;32m    164\u001b[0m                         TprogerParser('tproger')] \n\u001b[1;32m    165\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'not_processed_urls.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a0252de7f7a6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source_name)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         self.df = pd.read_csv(f\"{DATASETS_PATH}/{source_name}.csv\",\n\u001b[0m\u001b[1;32m     13\u001b[0m                          \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                          parse_dates=['post_time', 'parse_time'])\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/leo/DATASETS/habr.csv'"
     ]
    }
   ],
   "source": [
    "composer = ParserComposer()\n",
    "composer.pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-20 21:43:42 [scrapy.utils.log] INFO: Scrapy 2.5.0 started (bot: scrapybot)\n",
      "2021-04-20 21:43:42 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.2.0, Python 3.8.5 (default, Jan 27 2021, 15:41:15) - [GCC 9.3.0], pyOpenSSL 20.0.1 (OpenSSL 1.1.1k  25 Mar 2021), cryptography 3.4.7, Platform Linux-5.4.0-72-generic-x86_64-with-glibc2.29\n",
      "2021-04-20 21:43:42 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2021-04-20 21:43:42 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2021-04-20 21:43:42 [scrapy.extensions.telnet] INFO: Telnet Password: 7dc510bcf332c96b\n",
      "2021-04-20 21:43:42 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-04-20 21:43:42 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-04-20 21:43:42 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-04-20 21:43:42 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-04-20 21:43:42 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-04-20 21:43:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-04-20 21:43:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7f46b3397c70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "class HabrSpider(scrapy.Spider):\n",
    "    name = \"habr.com\"\n",
    "    start_urls = ['https://habr.com/ru/hubs/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for hub_url in response.css('.list-snippet__title-link'):\n",
    "            yield {\n",
    "                'hub_url': hub_url.css('a::attr(href)').getall()\n",
    "            }\n",
    "\n",
    "process = CrawlerProcess(get_project_settings())\n",
    "process.crawl(HabrSpider)\n",
    "#process.start()\n",
    "#process.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "\n",
    "full_months = ['января', 'февраля', 'марта',\n",
    "               'апреля', 'мая', 'июня',\n",
    "               'июля', 'августа', 'сентября',\n",
    "               'октября', 'ноября', 'декабря']\n",
    "\n",
    "short_months = [month_name[:3] for month_name in full_months]\n",
    "sm = '|'.join(short_months)\n",
    "\n",
    "\n",
    "def str_to_datetime(s):\n",
    "    '''Take string with date in russian and return datetime object'''\n",
    "    sentence = s.strip().lower().split()\n",
    "    now = datetime.datetime.now()\n",
    "    try:\n",
    "        year = int(re.search(r'\\d{4}', s).group(0))\n",
    "    except AttributeError:\n",
    "        year = now.year\n",
    "    try:\n",
    "        month = re.search(f'{sm}', s).group(0)\n",
    "        month = short_months.index(month) + 1\n",
    "        day = int(\n",
    "            re.search(r'(?<!\\d{2})\\d{1,2}(?=\\s)(?! дня|\\d+ |:\\d{2})', s).group(0))\n",
    "    except AttributeError:\n",
    "        if re.match(r'вчера', s):\n",
    "            yesterday = (now - datetime.timedelta(days=1))\n",
    "            month = yesterday.month\n",
    "            day = yesterday.day\n",
    "        elif re.match(r'позавчера', s):\n",
    "            yesterday2 = (now - datetime.timedelta(days=2))\n",
    "            month = yesterday2.month\n",
    "            day = yesterday2.day\n",
    "        elif re.match(r'завтра', s):\n",
    "            tomorrow = (now - datetime.timedelta(days=2))\n",
    "            month = tomorrow.month\n",
    "            day = tomorrow.day\n",
    "        else:\n",
    "            month = now.month\n",
    "            day = now.day\n",
    "    try:\n",
    "        hour = int(re.search(r'\\d{2}(?=:\\d{2})', s).group(0))\n",
    "        minute = int(re.search(r'(?<=\\d{2}:)\\d{2}', s).group(0))\n",
    "    except AttributeError:\n",
    "        if re.match(r'^\\d+(?= мин)', s):\n",
    "            t = now - \\\n",
    "                datetime.timedelta(minutes=int(\n",
    "                    re.match(r'\\d+(?= мин)', s).group(0)))\n",
    "        elif re.match(r'^\\d+(?= час)', s):\n",
    "            t = now - \\\n",
    "                datetime.timedelta(\n",
    "                    hours=int(re.match(r'\\d+(?= час)', s).group(0)))\n",
    "        else:\n",
    "            t = now\n",
    "        hour = t.hour\n",
    "        minute = t.minute\n",
    "    return datetime.datetime(year,\n",
    "                             month,\n",
    "                             day,\n",
    "                             hour,\n",
    "                             minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def habr_prepare(path=f'{DATASETS_PATH}/habr.json'):\n",
    "    #df_habr['filename'] = df['url'].apply(lambda x: str(uuid.uuid5(uuid.NAMESPACE_DNS, x)))\n",
    "    df_habr = pd.read_json(path).set_index('url')\n",
    "    df_habr = df_habr[~df_habr.index.duplicated(keep='first')]\n",
    "    df_habr.post_time = df_habr.post_time.apply(str_to_datetime)\n",
    "    df_habr.post_time = pd.to_datetime(df_habr.post_time)\n",
    "    df_habr.parse_time = pd.to_datetime('today')\n",
    "    df_habr['parse_time'] = pd.to_datetime(datetime.date.today())\n",
    "    df_habr.likes_num = df_habr.likes_num.apply(lambda x: x.replace('–', '-')).apply(int)\n",
    "    df_habr.views_num = df_habr.views_num.apply(lambda x: x.replace(',', '.').replace('k', 'e+3')).apply(float).apply(int)\n",
    "    df_habr.comments_num = df_habr.comments_num.fillna(0).apply(int)\n",
    "    df_habr.summary = df_habr.summary.apply(lambda x: re.sub(r'[\\n\\r\\s]{2,}', '\\n', x.replace('Читать дальше →', '').strip()))\n",
    "    df_habr['source'] = 'habr'\n",
    "    return df_habr\n",
    "\n",
    "df_habr = habr_prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>web-scraper-order</th>\n",
       "      <th>web-scraper-start-url</th>\n",
       "      <th>title</th>\n",
       "      <th>title-href</th>\n",
       "      <th>comments_num</th>\n",
       "      <th>summary</th>\n",
       "      <th>post_time</th>\n",
       "      <th>fulltext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1619373597-7730</td>\n",
       "      <td>https://dev.by/news?page=194</td>\n",
       "      <td>Netflix открыла код интерактивной среды вычисл...</td>\n",
       "      <td>https://dev.by/news/netflix-otkryla-kod-intera...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25 октября 2019, 15:59</td>\n",
       "      <td>Netflix представила новую интерактивную среду ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1619363449-3392</td>\n",
       "      <td>https://dev.by/news?page=334</td>\n",
       "      <td>30-летний мужчина из Минска, не использует нал...</td>\n",
       "      <td>https://dev.by/news/portret-belorusskogo-startapa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15 мая 2018, 14:15</td>\n",
       "      <td>Стартап-хаб Imaguru презентовал результаты пер...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1619376757-9014</td>\n",
       "      <td>https://dev.by/news?page=153</td>\n",
       "      <td>15 тысяч записей Zoom попали в открытый доступ</td>\n",
       "      <td>https://dev.by/news/15-tysyach-zapisei-zoom-po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Более 15 тысяч записей видеозвонков Zoom оказа...</td>\n",
       "      <td>4 апреля 2020, 12:07</td>\n",
       "      <td>Более 15 тысяч записей видеозвонков Zoom оказа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1619375125-8389</td>\n",
       "      <td>https://dev.by/news?page=173</td>\n",
       "      <td>Google запустила сервис для удобного поиска на...</td>\n",
       "      <td>https://dev.by/news/google-zapustila-servis-da...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24 января 2020, 09:31</td>\n",
       "      <td>Вышел из беты сервис Dataset Search для удобно...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1619362369-2950</td>\n",
       "      <td>https://dev.by/news?page=348</td>\n",
       "      <td>Apple начнёт уведомлять пользователей о сборе ...</td>\n",
       "      <td>https://dev.by/news/ios-ustroystva-budut-uvedo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30 марта 2018, 10:27</td>\n",
       "      <td>Обновлённые версии iOS 11.3 и macOS 10.13.5 по...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>1619380081-10348</td>\n",
       "      <td>https://dev.by/news?page=116</td>\n",
       "      <td>Директора «Хайв Проджект» (MolaMola, Ulej) уве...</td>\n",
       "      <td>https://dev.by/news/haiv-prodzhekt-dfr-i-kgb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Директора «Хайв Проджект» (MolaMola.by, Ulej.b...</td>\n",
       "      <td>4 августа 2020, 22:01</td>\n",
       "      <td>Директора «Хайв Проджект» (MolaMola.by, Ulej.b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>1619379973-10313</td>\n",
       "      <td>https://dev.by/news?page=117</td>\n",
       "      <td>Штаб Дмитриева запустил сервис, который считае...</td>\n",
       "      <td>https://dev.by/news/dmitriev-salary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кандидат в президенты Андрей Дмитриев сообщил ...</td>\n",
       "      <td>31 июля 2020, 16:04</td>\n",
       "      <td>Кандидат в президенты Андрей Дмитриев сообщил ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>1619379617-10201</td>\n",
       "      <td>https://dev.by/news?page=120</td>\n",
       "      <td>Что придумали 200+ разработчиков на хакатоне п...</td>\n",
       "      <td>https://dev.by/news/hacaton-idea-vote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18-19 июля в Минске прошёл хакатон Social Tech...</td>\n",
       "      <td>21 июля 2020, 17:16</td>\n",
       "      <td>18-19 июля в Минске прошёл хакатон Social Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>1619379591-10192</td>\n",
       "      <td>https://dev.by/news?page=121</td>\n",
       "      <td>История компании, которая уже 13 лет на удалёнке</td>\n",
       "      <td>https://dev.by/news/udalennaya-rabota-kak-chas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Концепция удаленной работы возникла гораздо ра...</td>\n",
       "      <td>21 июля 2020, 10:19</td>\n",
       "      <td>Концепция удаленной работы возникла гораздо ра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>1619380204-10384</td>\n",
       "      <td>https://dev.by/news?page=114</td>\n",
       "      <td>«Спастись от газа, настроить сеть». Чтение на ...</td>\n",
       "      <td>https://dev.by/news/besarab-chto-chitat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Сергей Бесараб, изобретатель, химик-исследоват...</td>\n",
       "      <td>7 августа 2020, 14:12</td>\n",
       "      <td>Сергей Бесараб, изобретатель, химик-исследоват...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15528 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    web-scraper-order         web-scraper-start-url  \\\n",
       "0     1619373597-7730  https://dev.by/news?page=194   \n",
       "1     1619363449-3392  https://dev.by/news?page=334   \n",
       "2     1619376757-9014  https://dev.by/news?page=153   \n",
       "3     1619375125-8389  https://dev.by/news?page=173   \n",
       "4     1619362369-2950  https://dev.by/news?page=348   \n",
       "..                ...                           ...   \n",
       "215  1619380081-10348  https://dev.by/news?page=116   \n",
       "216  1619379973-10313  https://dev.by/news?page=117   \n",
       "217  1619379617-10201  https://dev.by/news?page=120   \n",
       "218  1619379591-10192  https://dev.by/news?page=121   \n",
       "219  1619380204-10384  https://dev.by/news?page=114   \n",
       "\n",
       "                                                 title  \\\n",
       "0    Netflix открыла код интерактивной среды вычисл...   \n",
       "1    30-летний мужчина из Минска, не использует нал...   \n",
       "2       15 тысяч записей Zoom попали в открытый доступ   \n",
       "3    Google запустила сервис для удобного поиска на...   \n",
       "4    Apple начнёт уведомлять пользователей о сборе ...   \n",
       "..                                                 ...   \n",
       "215  Директора «Хайв Проджект» (MolaMola, Ulej) уве...   \n",
       "216  Штаб Дмитриева запустил сервис, который считае...   \n",
       "217  Что придумали 200+ разработчиков на хакатоне п...   \n",
       "218   История компании, которая уже 13 лет на удалёнке   \n",
       "219  «Спастись от газа, настроить сеть». Чтение на ...   \n",
       "\n",
       "                                            title-href comments_num  \\\n",
       "0    https://dev.by/news/netflix-otkryla-kod-intera...          NaN   \n",
       "1    https://dev.by/news/portret-belorusskogo-startapa          NaN   \n",
       "2    https://dev.by/news/15-tysyach-zapisei-zoom-po...          NaN   \n",
       "3    https://dev.by/news/google-zapustila-servis-da...          NaN   \n",
       "4    https://dev.by/news/ios-ustroystva-budut-uvedo...          NaN   \n",
       "..                                                 ...          ...   \n",
       "215       https://dev.by/news/haiv-prodzhekt-dfr-i-kgb          NaN   \n",
       "216                https://dev.by/news/dmitriev-salary          NaN   \n",
       "217              https://dev.by/news/hacaton-idea-vote          NaN   \n",
       "218  https://dev.by/news/udalennaya-rabota-kak-chas...          NaN   \n",
       "219            https://dev.by/news/besarab-chto-chitat          NaN   \n",
       "\n",
       "                                               summary  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2    Более 15 тысяч записей видеозвонков Zoom оказа...   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "215  Директора «Хайв Проджект» (MolaMola.by, Ulej.b...   \n",
       "216  Кандидат в президенты Андрей Дмитриев сообщил ...   \n",
       "217  18-19 июля в Минске прошёл хакатон Social Tech...   \n",
       "218  Концепция удаленной работы возникла гораздо ра...   \n",
       "219  Сергей Бесараб, изобретатель, химик-исследоват...   \n",
       "\n",
       "                  post_time                                           fulltext  \n",
       "0    25 октября 2019, 15:59  Netflix представила новую интерактивную среду ...  \n",
       "1        15 мая 2018, 14:15  Стартап-хаб Imaguru презентовал результаты пер...  \n",
       "2      4 апреля 2020, 12:07  Более 15 тысяч записей видеозвонков Zoom оказа...  \n",
       "3     24 января 2020, 09:31  Вышел из беты сервис Dataset Search для удобно...  \n",
       "4      30 марта 2018, 10:27  Обновлённые версии iOS 11.3 и macOS 10.13.5 по...  \n",
       "..                      ...                                                ...  \n",
       "215   4 августа 2020, 22:01  Директора «Хайв Проджект» (MolaMola.by, Ulej.b...  \n",
       "216     31 июля 2020, 16:04  Кандидат в президенты Андрей Дмитриев сообщил ...  \n",
       "217     21 июля 2020, 17:16  18-19 июля в Минске прошёл хакатон Social Tech...  \n",
       "218     21 июля 2020, 10:19  Концепция удаленной работы возникла гораздо ра...  \n",
       "219   7 августа 2020, 14:12  Сергей Бесараб, изобретатель, химик-исследоват...  \n",
       "\n",
       "[15528 rows x 8 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import dask.dataframe as dd\n",
    "import re\n",
    "\n",
    "def read_devby():\n",
    "    d = pd.concat(pd.read_csv(f'/home/leo/DATASETS/{f}') for f in listdir('/home/leo/DATASETS/'))\n",
    "    d = d.drop(columns='web-scraper-order')\n",
    "    d = d.rename(columns={'web-scraper-start-url':'url'}).set_index('url')\n",
    "    d = d.drop_duplicates(keep='last')\n",
    "    #months = ['янв', 'фев', 'мар', 'апр', 'мая', 'июн', 'июл', 'авг', 'сен', 'окт', 'ноя', 'дек']\n",
    "    #d['post_time'] = pd.to_datetime(d['post_time'].apply(lambda x: x.split()[2] + '-' + str(months.index(x.split()[1])+1)+ '-' + x.split()[0]))\n",
    "    #d['parse_time'] = pd.to_datetime('today').date()\n",
    "    #d['source'] = 'deby'\n",
    "    #d['num'] = d.index.map(lambda x: int(x.split('/')[-1]))\n",
    "    #sns.histplot(d.num)\n",
    "    #d = d.drop(columns='num')\n",
    "    return d\n",
    "\n",
    "d = read_devby()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>web-scraper-order</th>\n",
       "      <th>web-scraper-start-url</th>\n",
       "      <th>title</th>\n",
       "      <th>title-href</th>\n",
       "      <th>comments_num</th>\n",
       "      <th>summary</th>\n",
       "      <th>post_time</th>\n",
       "      <th>fulltext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1619373597-7730</td>\n",
       "      <td>https://dev.by/news?page=194</td>\n",
       "      <td>Netflix открыла код интерактивной среды вычисл...</td>\n",
       "      <td>https://dev.by/news/netflix-otkryla-kod-intera...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25 октября 2019, 15:59</td>\n",
       "      <td>Netflix представила новую интерактивную среду ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1619363449-3392</td>\n",
       "      <td>https://dev.by/news?page=334</td>\n",
       "      <td>30-летний мужчина из Минска, не использует нал...</td>\n",
       "      <td>https://dev.by/news/portret-belorusskogo-startapa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15 мая 2018, 14:15</td>\n",
       "      <td>Стартап-хаб Imaguru презентовал результаты пер...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1619376757-9014</td>\n",
       "      <td>https://dev.by/news?page=153</td>\n",
       "      <td>15 тысяч записей Zoom попали в открытый доступ</td>\n",
       "      <td>https://dev.by/news/15-tysyach-zapisei-zoom-po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Более 15 тысяч записей видеозвонков Zoom оказа...</td>\n",
       "      <td>4 апреля 2020, 12:07</td>\n",
       "      <td>Более 15 тысяч записей видеозвонков Zoom оказа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1619375125-8389</td>\n",
       "      <td>https://dev.by/news?page=173</td>\n",
       "      <td>Google запустила сервис для удобного поиска на...</td>\n",
       "      <td>https://dev.by/news/google-zapustila-servis-da...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24 января 2020, 09:31</td>\n",
       "      <td>Вышел из беты сервис Dataset Search для удобно...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1619362369-2950</td>\n",
       "      <td>https://dev.by/news?page=348</td>\n",
       "      <td>Apple начнёт уведомлять пользователей о сборе ...</td>\n",
       "      <td>https://dev.by/news/ios-ustroystva-budut-uvedo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30 марта 2018, 10:27</td>\n",
       "      <td>Обновлённые версии iOS 11.3 и macOS 10.13.5 по...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>1619380081-10348</td>\n",
       "      <td>https://dev.by/news?page=116</td>\n",
       "      <td>Директора «Хайв Проджект» (MolaMola, Ulej) уве...</td>\n",
       "      <td>https://dev.by/news/haiv-prodzhekt-dfr-i-kgb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Директора «Хайв Проджект» (MolaMola.by, Ulej.b...</td>\n",
       "      <td>4 августа 2020, 22:01</td>\n",
       "      <td>Директора «Хайв Проджект» (MolaMola.by, Ulej.b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>1619379973-10313</td>\n",
       "      <td>https://dev.by/news?page=117</td>\n",
       "      <td>Штаб Дмитриева запустил сервис, который считае...</td>\n",
       "      <td>https://dev.by/news/dmitriev-salary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кандидат в президенты Андрей Дмитриев сообщил ...</td>\n",
       "      <td>31 июля 2020, 16:04</td>\n",
       "      <td>Кандидат в президенты Андрей Дмитриев сообщил ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>1619379617-10201</td>\n",
       "      <td>https://dev.by/news?page=120</td>\n",
       "      <td>Что придумали 200+ разработчиков на хакатоне п...</td>\n",
       "      <td>https://dev.by/news/hacaton-idea-vote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18-19 июля в Минске прошёл хакатон Social Tech...</td>\n",
       "      <td>21 июля 2020, 17:16</td>\n",
       "      <td>18-19 июля в Минске прошёл хакатон Social Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>1619379591-10192</td>\n",
       "      <td>https://dev.by/news?page=121</td>\n",
       "      <td>История компании, которая уже 13 лет на удалёнке</td>\n",
       "      <td>https://dev.by/news/udalennaya-rabota-kak-chas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Концепция удаленной работы возникла гораздо ра...</td>\n",
       "      <td>21 июля 2020, 10:19</td>\n",
       "      <td>Концепция удаленной работы возникла гораздо ра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>1619380204-10384</td>\n",
       "      <td>https://dev.by/news?page=114</td>\n",
       "      <td>«Спастись от газа, настроить сеть». Чтение на ...</td>\n",
       "      <td>https://dev.by/news/besarab-chto-chitat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Сергей Бесараб, изобретатель, химик-исследоват...</td>\n",
       "      <td>7 августа 2020, 14:12</td>\n",
       "      <td>Сергей Бесараб, изобретатель, химик-исследоват...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15528 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    web-scraper-order         web-scraper-start-url  \\\n",
       "0     1619373597-7730  https://dev.by/news?page=194   \n",
       "1     1619363449-3392  https://dev.by/news?page=334   \n",
       "2     1619376757-9014  https://dev.by/news?page=153   \n",
       "3     1619375125-8389  https://dev.by/news?page=173   \n",
       "4     1619362369-2950  https://dev.by/news?page=348   \n",
       "..                ...                           ...   \n",
       "215  1619380081-10348  https://dev.by/news?page=116   \n",
       "216  1619379973-10313  https://dev.by/news?page=117   \n",
       "217  1619379617-10201  https://dev.by/news?page=120   \n",
       "218  1619379591-10192  https://dev.by/news?page=121   \n",
       "219  1619380204-10384  https://dev.by/news?page=114   \n",
       "\n",
       "                                                 title  \\\n",
       "0    Netflix открыла код интерактивной среды вычисл...   \n",
       "1    30-летний мужчина из Минска, не использует нал...   \n",
       "2       15 тысяч записей Zoom попали в открытый доступ   \n",
       "3    Google запустила сервис для удобного поиска на...   \n",
       "4    Apple начнёт уведомлять пользователей о сборе ...   \n",
       "..                                                 ...   \n",
       "215  Директора «Хайв Проджект» (MolaMola, Ulej) уве...   \n",
       "216  Штаб Дмитриева запустил сервис, который считае...   \n",
       "217  Что придумали 200+ разработчиков на хакатоне п...   \n",
       "218   История компании, которая уже 13 лет на удалёнке   \n",
       "219  «Спастись от газа, настроить сеть». Чтение на ...   \n",
       "\n",
       "                                            title-href comments_num  \\\n",
       "0    https://dev.by/news/netflix-otkryla-kod-intera...          NaN   \n",
       "1    https://dev.by/news/portret-belorusskogo-startapa          NaN   \n",
       "2    https://dev.by/news/15-tysyach-zapisei-zoom-po...          NaN   \n",
       "3    https://dev.by/news/google-zapustila-servis-da...          NaN   \n",
       "4    https://dev.by/news/ios-ustroystva-budut-uvedo...          NaN   \n",
       "..                                                 ...          ...   \n",
       "215       https://dev.by/news/haiv-prodzhekt-dfr-i-kgb          NaN   \n",
       "216                https://dev.by/news/dmitriev-salary          NaN   \n",
       "217              https://dev.by/news/hacaton-idea-vote          NaN   \n",
       "218  https://dev.by/news/udalennaya-rabota-kak-chas...          NaN   \n",
       "219            https://dev.by/news/besarab-chto-chitat          NaN   \n",
       "\n",
       "                                               summary  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2    Более 15 тысяч записей видеозвонков Zoom оказа...   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "215  Директора «Хайв Проджект» (MolaMola.by, Ulej.b...   \n",
       "216  Кандидат в президенты Андрей Дмитриев сообщил ...   \n",
       "217  18-19 июля в Минске прошёл хакатон Social Tech...   \n",
       "218  Концепция удаленной работы возникла гораздо ра...   \n",
       "219  Сергей Бесараб, изобретатель, химик-исследоват...   \n",
       "\n",
       "                  post_time                                           fulltext  \n",
       "0    25 октября 2019, 15:59  Netflix представила новую интерактивную среду ...  \n",
       "1        15 мая 2018, 14:15  Стартап-хаб Imaguru презентовал результаты пер...  \n",
       "2      4 апреля 2020, 12:07  Более 15 тысяч записей видеозвонков Zoom оказа...  \n",
       "3     24 января 2020, 09:31  Вышел из беты сервис Dataset Search для удобно...  \n",
       "4      30 марта 2018, 10:27  Обновлённые версии iOS 11.3 и macOS 10.13.5 по...  \n",
       "..                      ...                                                ...  \n",
       "215   4 августа 2020, 22:01  Директора «Хайв Проджект» (MolaMola.by, Ulej.b...  \n",
       "216     31 июля 2020, 16:04  Кандидат в президенты Андрей Дмитриев сообщил ...  \n",
       "217     21 июля 2020, 17:16  18-19 июля в Минске прошёл хакатон Social Tech...  \n",
       "218     21 июля 2020, 10:19  Концепция удаленной работы возникла гораздо ра...  \n",
       "219   7 августа 2020, 14:12  Сергей Бесараб, изобретатель, химик-исследоват...  \n",
       "\n",
       "[15528 rows x 8 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
